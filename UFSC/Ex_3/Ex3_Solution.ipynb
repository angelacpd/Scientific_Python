{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidade Federal de Santa Catarina<br>\n",
    "Departamento de Engenharia Elétrica e Eletrônica<br>\n",
    "EEL7514/EEL7513/EEL410250 - Aprendizado de Máquina\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\bzero}{\\mathbf{0}}$\n",
    "\n",
    "\n",
    "# Exercício 3: Regressão Linear & Otimização Numérica\n",
    "\n",
    "Neste exercício você irá explorar métodos de otimização numérica para treinar um modelo de regressão linear. Em particular, você irá implementar o método do gradiente e analisar sua convergência. Além disso, você irá investigar o efeito da normalização de atributos no comportamento do método. Finalmente, você irá investigar a aplicação de regressão linear em um conjunto de dados real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunto de dados #1\n",
    "\n",
    "Inicialmente, utilizaremos o mesmo conjunto de dados do exercício anterior, exceto por uma escala diferente em $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def gen_data(n_samples, x_scale=[0,1], noise=0.5):\n",
    "    '''Generate univariate regression dataset'''\n",
    "    x = np.sort(np.random.rand(n_samples))\n",
    "    y = 6*(-1/6 + x + (x > 1/3)*(2/3-2*x) + (x > 2/3)*(2*x-4/3)) + noise*np.random.randn(n_samples)\n",
    "    x = x_scale[0] + (x_scale[1]-x_scale[0])*x\n",
    "    X = x.reshape(-1,1)\n",
    "    return X, y\n",
    "\n",
    "def plot_data(X, y):\n",
    "    '''Plot univariate regression dataset'''\n",
    "    assert len(X.shape) == 2 and len(y.shape) == 1\n",
    "    plt.plot(X[:,0],y,'b.'); plt.xlabel('x'); plt.ylabel('y');\n",
    "    return\n",
    "\n",
    "def plot_prediction(model, X, y, n_points=100):\n",
    "    '''Plot dataset and predictions for a univariate regression model'''\n",
    "    plot_data(X,y)\n",
    "    if n_points is not None:\n",
    "        xx = np.linspace(X.min(),X.max(),n_points)\n",
    "        yy = model.predict(xx.reshape(-1,1))\n",
    "        plt.plot(xx,yy,'r-')\n",
    "    y_pred = model.predict(X)\n",
    "    plt.plot(X[:,0],y_pred,'r.')\n",
    "    plt.legend(['True', 'Predicted'])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados pode ser gerado e visualizado pelos comandos abaixo (observe a nova escala)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1) (30,)\n",
      "(1000, 1) (1000,)\n",
      "(1000, 1) (1000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(2019*2)\n",
    "X, y = gen_data(n_samples=30, x_scale=[0,4])\n",
    "X_val, y_val = gen_data(n_samples=1000, x_scale=[0,4])\n",
    "X_test, y_test = gen_data(n_samples=1000, x_scale=[0,4])\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Plot only the training data!\n",
    "plot_data(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Método do gradiente\n",
    "\n",
    "Resgate a classe do modelo que você implementou no exercício anterior. Iremos reorganizá-la para permitir um método de treinamento alternativo.\n",
    "\n",
    "1. Utilize a classe abaixo, substituindo na função `_fit_ne` a sua função `fit` implementada anteriormente, com as modificações necessárias. Note que a função `add_powers` foi eliminada (bem como o argumento `d` da inicialização do modelo), sendo substituída pela função `_add_ones` (que simplesmente adiciona uma coluna de 1's). Ou seja, nosso modelo deve implementar puramente uma regressão linear (com regularização L2), sem atributos adicionais. Caso desejemos atributos polinomiais, poderemos usar a classe `PolynomialFeatures` do sklearn. A única vantagem do nosso modelo de regressão próprio em relação ao `Ridge` é permitir utilizar um método de treinamento diferente.\n",
    "- Mova a função `mse` para fora da classe, caso contrário não poderemos acessá-la dentro de um `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    # Linear regression with L2 regularization\n",
    "    def __init__(self, lamb=0, solver='ne', lr=1, maxiter=1000, tol=1e-5):\n",
    "        # Initialization\n",
    "        self.lamb = lamb\n",
    "        self.solver = solver\n",
    "        self.lr = lr\n",
    "        self.maxiter = maxiter\n",
    "        self.tol = tol\n",
    "        return\n",
    "    \n",
    "    def _add_ones(self, X):\n",
    "        # Add column of ones\n",
    "        X_new = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X_new\n",
    "    \n",
    "    def _fit_ne(self, X, y):\n",
    "        X = self._add_ones(X)\n",
    "        L = ???\n",
    "        self.hessian = ???\n",
    "        assert np.linalg.matrix_rank(X.T @ X + self.lamb*L) == X.shape[1], 'Singular matrix'\n",
    "        self.w = ???\n",
    "        return\n",
    "\n",
    "    def _fit_gd(self, X, y):\n",
    "        # Fit by gradient descent\n",
    "        ???\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.solver == 'gd':\n",
    "            self._fit_gd(X, y)\n",
    "        elif self.solver == 'ne':\n",
    "            self._fit_ne(X, y)\n",
    "        else:\n",
    "            raise RuntimeError('Unknown solver')\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self._add_ones(X)\n",
    "        y_pred = ???\n",
    "        return y_pred\n",
    "\n",
    "def mse(model, X, y):\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Modifique a função `_fit_ne` para calcular também a matriz hessiana da função custo (regularizada), guardando-a na variável `self.hessian`. Em seguida, após o treinamento usando a solução analítica, estime o grau de condicionamento da hessiana utilizana a função `np.linalg.cond()`.\n",
    "- Complete a função `_fit_gd` implementando o método do gradiente. Utilize os parâmetros `self.lr` (taxa de aprendizado), `self.maxiter` (número máximo de iterações) e `self.tol` (critério de parada para a norma do gradiente), e assuma como ponto inicial $\\bw = (0,\\ldots,0)$. Além de calcular `self.w`, sua função deve criar também uma lista, `self.J_history`, contendo os valores da função custo (regularizada) a cada iteração, a qual será usada para monitorar o treinamento e analisar a taxa de aprendizado.\n",
    "- Treine o modelo sem regularização usando `solver='gd'`, trace o gráfico de `J_history` e escolha uma boa taxa de aprendizado. Quantas iterações foram necessárias para convergência?\n",
    "- Calcule o MSE de treinamento e de validação e compare-os com os obtidos pela solução analítica. Compare também os vetores $\\bw$ das duas soluções. (Obs: a saída da célula 5 está mostrada apenas para ilustração. Não é necessário reproduzir exatamente o mesmo texto/gráfico.)\n",
    "- (OPCIONAL) O que acontece com o erro de validação à medida que a taxa de aprendizado é reduzida? Como podemos interpretar esse fenômeno?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [-0.20189161  0.02333163]\n",
      "Train MSE: 0.517264\n",
      "  Val MSE: 0.601059\n",
      "Condition number: 24.959359\n"
     ]
    }
   ],
   "source": [
    "# Normal equation\n",
    "model = Model()\n",
    "model.fit(X, y)\n",
    "print('w =', ???)\n",
    "print('Train MSE: %f' % mse(model, X, y));\n",
    "print('  Val MSE: %f' % mse(model, X_val, y_val));\n",
    "print('Condition number: %f' % ???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [-0.20187165  0.0233235 ]\n",
      "Train MSE: 0.517264\n",
      "  Val MSE: 0.601058\n",
      "Iterations: 631\n",
      "MAPE from optimal w: 0.022371%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAr3klEQVR4nO3deZgV5Z328e9N24DiBoLKiAHMECMoNNASOpqAIq64jkbMJEJMhmCCxCwqSeYyxugbt1HHZTQY14QRjQYkGRzjhoKi0iASQInEQW1FRFQQo0LD7/3jVDeH5vRC96nTwLk/11XXqXpqe6rUvn1qeUoRgZmZWb61ae0KmJnZjskBY2ZmqXDAmJlZKhwwZmaWCgeMmZmlwgFjZmapcMCY5ZmkuyRd1tr1MGttDhiz7YCkgyU9Iuk9SX55zbYLDhiz7cN64H7g261dEbOm2qm1K2C2vZPUH7gd6AVMB/LewoiIJcASSf+c722bpcUtGLMWkNQWmAr8DugE/AH4lwaWP1zShw0Mhxem5mbpcwvGrGUGA6XA9ZHp2O8BST+qb+GImAXsWaC6mbUqt2DMWuafgLdi815jX2+typhtSxwwZi2zHNhPkrLKPlffwpK+ImltA8NX0q+yWWE4YMxaZjZQDYyXtJOk04BB9S0cETMjYtcGhpm51lNGe6BtMt1eUrs0DsgsXxwwZi0QEeuA04DRwAfAmcAfU9hVd+ATYFEy/QmwJIX9mOWN/MExMzNLg1swZmaWilQDRtKxkpZIWippQo75QyWtljQ/GS7OmrdM0l+T8sqs8k6SHpX0avLbMc1jMDOz5kntEpmkEuBvwHCgCpgDnBURi7OWGQr8JCJG5Fh/GVAeEe/VKb8KeD8irkhCq2NEXJTKQZiZWbOl2YIZBCyNiNeSG6GTgZPzsN2TgbuT8buBU/KwTTMzy7M03+TfD3gza7oK+FKO5SokvQS8TaY1U/OUTAB/SXqO/U1ETEzK94mI5QARsVzS3rl2LmkMMAagQ4cOA7/4xS+2+IDMzIrJ3Llz34uILs1dP82AUY6yutfj5gHdI2KtpOPJ9OnUK5l3WES8nQTIo5JeiYinm7rzJJAmApSXl0dlZWUja5iZWTZJLeqVIs1LZFXA/lnT3ci0UmpFxJqIWJuMTwdKJXVOpt9Oft8FprDp5bUVkroCJL/vpngMZmbWTGkGzBygl6SeSY+zI4Fp2QtI2remiw1Jg5L6rJLUQdJuSXkH4GhgYbLaNGBUMj4KeCjFYzAzs2ZK7RJZRFRLGgc8ApQAd0TEIkljk/m3AqcD50qqJvNm8siICEn7AFOS7NkJ+O+I+N9k01cA90v6NvAGcEZax2BmZs1XFG/y+x6MWfOsX7+eqqoqPv3009auiqWoffv2dOvWjdLS0s3KJc2NiPLmbtffgzGzelVVVbHbbrvRo0cPNu8w2nYUEcGqVauoqqqiZ8+eed22u4oxs3p9+umn7LXXXg6XHZgk9tprr1RaqQ4YM2uQw2XHl9Y/YweMmZmlwgFjZts0Sfz4xz+unb7mmmu45JJLClqHoUOH0loPCl1//fX84x//qJ0+/vjj+fDDD1ulLlvLAWNm27R27drxxz/+kffee6/xhXOorq7Oc40Kq27ATJ8+nT333LP1KrQVHDBmtk3baaedGDNmDNddd90W815//XWGDRtG3759GTZsGG+88QYAo0eP5kc/+hFHHHEEF110EaNHj+bcc8/liCOO4IADDuCpp57inHPO4aCDDmL06NG12zv33HMpLy+nT58+/OIXv2i0bnPnzmXIkCEMHDiQY445huXLl9eW9+vXj4qKCi644AIOPvhgAO666y7GjRtXu/6IESOYMWNGvfu+4YYbePvttzniiCM44ogjAOjRo0dt2F577bUcfPDBHHzwwVx//fUALFu2jIMOOoh/+7d/o0+fPhx99NF88sknW3HG88ePKZtZ05x/Psyfn99tlpVB8oexId///vfp27cvF1544Wbl48aN4+yzz2bUqFHccccdjB8/nqlTpwLwt7/9jccee4ySkhJGjx7NBx98wBNPPMG0adM48cQTeeaZZ/jtb3/LoYceyvz58ykrK+Pyyy+nU6dObNiwgWHDhrFgwQL69u2bs07r16/nvPPO46GHHqJLly7cd999/PznP+eOO+7gW9/6FjfeeCNDhgzhggsuaNKpyLXv8ePHc+211/Lkk0/SuXPnzZafO3cud955J88//zwRwZe+9CWGDBlCx44defXVV7n33nu57bbb+NrXvsaDDz7IN77xjSbVI5/cgjGzbd7uu+/O2WefzQ033LBZ+ezZs/n6178OwDe/+U1mzZpVO++MM86gpKSkdvrEE09EEocccgj77LMPhxxyCG3atKFPnz4sW7YMgPvvv58BAwbQv39/Fi1axOLFi6nPkiVLWLhwIcOHD6esrIzLLruMqqoqVq9ezYcffsiQIUNq69UUW7NvgFmzZnHqqafSoUMHdt11V0477TRmzpwJQM+ePSkrKwNg4MCBtcdXaG7BmFnTNKGlkabzzz+fAQMG8K1vfaveZbIft+3QocNm89q1awdAmzZtasdrpqurq/m///s/rrnmGubMmUPHjh0ZPXp0g++GRAR9+vRh9uzZm5V/+OGH9T72u9NOO7Fx48ba6Zrtb+2+a/Zfn+zjKykpabVLZG7BmNl2oVOnTnzta1/j9ttvry378pe/zOTJkwGYNGkShx9+eLO3v2bNGjp06MAee+zBihUrePjhhxtc/sADD2TlypW1AbN+/XoWLVrEnnvuyR577FHbmpo0aVLtOj169GD+/Pls3LiRN998kxdeeKHRfe+222589NFHW+z/q1/9KlOnTuUf//gHH3/8MVOmTOErX/lKs48/DW7BmNl248c//jE33XRT7fQNN9zAOeecw9VXX02XLl248847m73tfv360b9/f/r06cMBBxzAYYcd1uDybdu25YEHHmD8+PGsXr2a6upqzj//fPr06cOdd97JOeecwy677MIxxxxTu85hhx1Gz549OeSQQzj44IMZMGBAo/seM2YMxx13HF27duXJJ5+sLR8wYACjR49m0KDMl0y+853v0L9//1a7HJaLO7s0s3q9/PLLHHTQQa1dje3asmXLGDFiBAsXLmx84VaU6591Szu79CUyMzNLhQPGzCxFPXr02OZbL2lxwJiZWSocMGZmlgoHjJmZpSLVgJF0rKQlkpZKmpBj/lBJqyXNT4aL68wvkfSipD9nlV0i6a2sdY5P8xjMzKx5UgsYSSXAzcBxQG/gLEm9cyw6MyLKkuHSOvN+ALycY53rstaZnt+am9m2ZMWKFXz961/ngAMOYODAgVRUVDBlypQWbfOSSy7hmmuuAeDiiy/msccea9Z25s+fz/TpTfsT1Jpd/gNMnTq10e5n8i3NFswgYGlEvBYR64DJwMlNXVlSN+AE4Lcp1c/MtnERwSmnnMJXv/pVXnvtNebOncvkyZOpqqraYtnmdst/6aWXctRRRzVr3a0JmNa2owXMfsCbWdNVSVldFZJekvSwpD5Z5dcDFwIbc6wzTtICSXdI6pi3GptZi82eDb/+dea3pZ544gnatm3L2LFja8u6d+/OeeedB2S6vz/jjDM48cQTOfroo1m7di3Dhg1jwIABHHLIITz00EO1611++eUceOCBHHXUUSxZsqS2fPTo0TzwwANA/d3vDx06lIsuuohBgwbxhS98gZkzZ7Ju3Touvvhi7rvvPsrKyrjvvvs2q/snn3zCyJEj6du3L2eeeeZm/YH95S9/oaKiggEDBnDGGWewdu1aACZMmEDv3r3p27cvP/nJT4BMC+7UU0+lX79+9OvXj2effRaA3//+9wwaNIiysjK++93vsmHDBgB23XVXfv7zn9OvXz8GDx7MihUrePbZZ5k2bRoXXHABZWVl/P3vf2/5P5wmSLOrmFy9vdXtNmAe0D0i1ib3UqYCvSSNAN6NiLmShtZZ5xbgV8m2fgX8B3DOFjuXxgBjAD73uc81/yjMDGhab/2rV8OCBbBxI7RpA337wh571L98Y731L1q0qLY7lfrMnj2bBQsW0KlTJ6qrq5kyZQq777477733HoMHD+akk05i3rx5TJ48mRdffJHq6moGDBjAwIEDN9tOQ93vQ6aF9MILLzB9+nR++ctf8thjj3HppZdSWVm5Wfc1NW655RZ22WUXFixYwIIFC2qP47333uOyyy7jscceo0OHDlx55ZVce+21jBs3jilTpvDKK68gqfarlePHj2fIkCFMmTKFDRs2sHbtWl5++WXuu+8+nnnmGUpLS/ne977HpEmTOPvss/n4448ZPHgwl19+ORdeeCG33XYb//7v/85JJ53EiBEjOP300xs8n/mUZsBUAftnTXcD3s5eICLWZI1Pl/RfkjoDhwEnJaHTHthd0u8j4hsRsaJmHUm3AX8mh4iYCEyETFcxeTomM2vA6tWZcIHM7+rVDQfM1vr+97/PrFmzaNu2LXPmzAFg+PDhdOrUCchcUvvZz37G008/TZs2bXjrrbdYsWIFM2fO5NRTT2WXXXYB4KSTTtpi29nd7wNs2LCBrl271s4/7bTTgKZ3f//0008zfvx4APr27Vv7XZnnnnuOxYsX1/Y3tm7dOioqKth9991p37493/nOdzjhhBMYMWIEkGnF3XPPPUCmZ+Q99tiD3/3ud8ydO5dDDz0UyLSW9t57byDTR1rNugMHDuTRRx9t0rlNQ5oBM4dMa6Qn8BYwEvh69gKS9gVWRERIGkTmkt2qiPgp8NNkmaHATyLiG8l014hYnmziVKA4X5E1K7Cm9NY/ezYMGwbr1kHbtjBpElRUNH+fffr04cEHH6ydvvnmm3nvvfcoL9/UPVZ2t/yTJk1i5cqVzJ07l9LSUnr06FHb7X19XejXqK/7/Ro1XeCXlJQ0+X5Prn1GBMOHD+fee+/dYt4LL7zA448/zuTJk7npppt44okn6q3rqFGj+PWvf73FvNLS0tr9bk1d05DaPZiIqAbGAY+QeRLs/ohYJGmspJoLqqcDCyW9BNwAjIzGe9+8StJfJS0AjgB+mNIhmNlWqqiAxx+HX/0q89uScAE48sgj+fTTT7nllltqy7K/T1/X6tWr2XvvvSktLeXJJ5/k9ddfBzJd20+ZMoVPPvmEjz76iD/96U9brFtf9/sNqa8r/Zp91nTVv3DhQhYsWADA4MGDeeaZZ1i6dGnt8fztb39j7dq1rF69muOPP57rr7+e+cn1yGHDhtUe/4YNG1izZg3Dhg3jgQce4N133wXg/fffrz3W5tQ1Lam+BxMR0yPiCxHx+Yi4PCm7NSJuTcZviog+EdEvIgZHxLM5tjEjIkZkTX8zIg6JiL4RcVJWa8bMtgEVFfDTn7Y8XCDTApg6dSpPPfUUPXv2ZNCgQYwaNYorr7wy5/L/+q//SmVlJeXl5UyaNIkvfvGLQKZr+zPPPJOysjL+5V/+Jed3U2q637/ooovo168fZWVltTfU63PEEUewePHinDf5zz33XNauXUvfvn256qqrarvV79KlC3fddRdnnXUWffv2ZfDgwbzyyit89NFHjBgxgr59+zJkyBCuu+46AP7zP/+TJ598kkMOOYSBAweyaNEievfuzWWXXcbRRx9N3759GT58eO0DCfUZOXIkV199Nf379y/YTX53129m9XJ3/cXD3fWbmdl2wwFjZmapcMCYWYOK4TJ6sUvrn3FRBMw77+TnrWKzYtO+fXtWrVrlkNmBRQSrVq2iffv2ed92Udzkl8pj550r8/LYpFkxWb9+PVVVVbXvktiOqX379nTr1o3S0tLNylt6kz/NFy23KevWwYwZDhizrVFaWkrPnj1buxq2nSqKS2SQeat46NDWroWZWfEoioBp1y4/bxWbmVnTFUXAtG3rcDEzK7SiCJgieI7BzGybUxQBY2ZmhVcUAeMWjJlZ4TlgzMwsFUURMGZmVnhFETBuwZiZFV5RBIyZmRVeUQSMWzBmZoXngDEzs1SkGjCSjpW0RNJSSRNyzB8qabWk+clwcZ35JZJelPTnrLJOkh6V9Gry2zHNYzAzs+ZJLWAklQA3A8cBvYGzJPXOsejMiChLhkvrzPsB8HKdsgnA4xHRC3g8mW6QWzBmZoWXZgtmELA0Il6LiHXAZODkpq4sqRtwAvDbOrNOBu5Oxu8GTmlsWw4YM7PCSzNg9gPezJquSsrqqpD0kqSHJfXJKr8euBDYWGf5fSJiOUDyu3eunUsaI6lSUuXGjXU3YWZmaUszYJSjrG5bYh7QPSL6ATcCUwEkjQDejYi5zd15REyMiPKIKJeK4lkGM7NtSpp/eauA/bOmuwFvZy8QEWsiYm0yPh0oldQZOAw4SdIyMpfWjpT0+2S1FZK6AiS/76Z4DGZm1kxpBswcoJeknpLaAiOBadkLSNpXkpLxQUl9VkXETyOiW0T0SNZ7IiK+kaw2DRiVjI8CHmqsIr4HY2ZWeDulteGIqJY0DngEKAHuiIhFksYm828FTgfOlVQNfAKMjGg0Dq4A7pf0beAN4IzG69KCAzEzs2ZR43/Pt38lJeWxYUNla1fDzGy7ImluRJQ3d/2iuPtdBBlqZrbNccCYmVkqiiJgzMys8IomYNyKMTMrrKIJmA0bWrsGZmbFxQFjZmapcMCYmVkqiiZgqqtbuwZmZsWlaALGLRgzs8JywJiZWSocMGZmlgoHjJmZpcIBY2ZmqXDAmJlZKhwwZmaWCgeMmZmlomgCxi9ampkVVtEEjFswZmaFlWrASDpW0hJJSyVNyDF/qKTVkuYnw8VJeXtJL0h6SdIiSb/MWucSSW9lrXN8U+rigDEzK6yd0tqwpBLgZmA4UAXMkTQtIhbXWXRmRIyoU/YZcGRErJVUCsyS9HBEPJfMvy4irtma+jhgzMwKK80WzCBgaUS8FhHrgMnAyU1ZMTLWJpOlydCiT4Y5YMzMCivNgNkPeDNruiopq6siuRT2sKQ+NYWSSiTNB94FHo2I57PWGSdpgaQ7JHXMtXNJYyRVSqoEB4yZWaGlGTDKUVa3FTIP6B4R/YAbgam1C0ZsiIgyoBswSNLByaxbgM8DZcBy4D9y7TwiJkZEeUSUgwPGzKzQ0gyYKmD/rOluwNvZC0TEmppLYRExHSiV1LnOMh8CM4Bjk+kVSfhsBG4jcymuUQ4YM7PCSjNg5gC9JPWU1BYYCUzLXkDSvpKUjA9K6rNKUhdJeyblOwNHAa8k012zNnEqsLAplXHAmJkVVmpPkUVEtaRxwCNACXBHRCySNDaZfytwOnCupGrgE2BkREQSIncnT6K1Ae6PiD8nm75KUhmZy23LgO82pT5+0dLMrLAUkfvhLEkXRsRVyfgZEfGHrHn/LyJ+VqA6tphUHtOnV3Lcca1dEzOz7YekuTX3sZujoUtkI7PGf1pn3rHN3WFr8SUyM7PCaihgVM94rultngPGzKywGgqYqGc81/Q2zwFjZlZYDd3k7ydpDZnWys7JOMl0+9RrlmcOGDOzwqo3YCKipJAVSZsDxsyssOq9RCZpl6SjyZrpAyX9UNKphalafk2ZArNnt3YtzMyKR0P3YP4X6AEg6Z+B2cABZPoBuyL9quXXH/4Aw4Y5ZMzMCqWhgOkYEa8m46OAeyPiPOA44ITUa5ZnEbBuHcyY0do1MTMrDk19iuxI4FGApOv9jWlWKg1t2kDbtjB0aGvXxMysODQUMAskXSPph8A/A38BqOkjbHtzzDHw+ONQUdHaNTEzKw4NBcy/Ae+RuQ9zdET8IynvDWzV1yS3BYcd5nAxMyukhh5T/gTY4mZ+RDwLPJtmpdLw2WetXQMzs+JSb8BIWtDQihHRN//VSYcEn37a2rUwMysuDb3Jv5HMjf7/Bv5Epjv97VKbNm7BmJkVWr33YJLPFZ8F7EomZC4H+gBvRcTrBaldnkgOGDOzQmvwi5YR8UpE/CIiBpBpxdwD/LAgNcsjB4yZWeE1+EVLSfuR+S7MqcAHZMJlSgHqlVdt2vgejJlZoTV0k/8pYDfgfmA08H4yq62kThHxfn3rbmvcgjEzK7yGLpF1BzqS+eb9X4DKZJib/DZK0rGSlkhaKmlCjvlDJa2WND8ZLk7K20t6QdJLkhZJ+mXWOp0kPSrp1eS3Y6MH6Zv8ZmYF19B7MD1asmFJJcDNwHCgCpgjaVpELK6z6MyIGFGn7DPgyIhYm/ToPEvSwxHxHDABeDwirkhCawJwUcN1ccCYmRVagzf5W2gQsDQiXkv6L5sMnNyUFSNjbTJZmgw1faOdDNydjN8NnNLY9nwPxsys8NIMmP2AN7Omq5KyuiqSS2EPS+pTUyipRNJ84F3g0Yh4Ppm1T0QsB0h+9861c0ljJFVKqly/fp1bMGZmBZZmwChHWdSZngd0j4h+wI3A1NoFIzYk7+J0AwZJOnhrdh4REyOiPCLK27dv64AxMyuwJgVM0pr4J0mfqxmasFoVsH/WdDfg7ewFImJNzaWwiJgOlErqXGeZD4EZwLFJ0QpJXZN6dSXTwmmk/r5EZmZWaI0GjKTzgBVkvgfzP8nw5yZsew7QS1JPSW3JvE8zrc6295WkZHxQUp9VkrrUfBZA0s7AUcAryWrTyHwAjeT3ocYq4qfIzMwKr8EXLRM/AA6MiFVbs+GIqJY0DngEKAHuiIhFksYm828FTgfOlVRNpq+zkRERScvk7uRJtDbA/RFRE2pXAPdL+jbwBnBGY3XxU2RmZoWniLq3ReosID0JDI+I6sJUKf/22ac81q2r5IMPWrsmZmbbD0lzI6K8ues3pQXzGjBD0v+QeT8FgIi4trk7LTTfgzEzK7ym3OR/g8z9l7Zkuo6pGbYb1dWZgHl2u/tMmpnZ9qvRS2Q7Aqk8oJKdd4bHH/enk83MmiK1S2SSro+I8yX9iS3fXyEiTmruTlvLunUwY4YDxsysEBq6B/O75PeaQlQkTRJEQGkpDB3a2rUxMysODXV2OTf5fapw1UnHP/0TvPUW3HKLWy9mZoXS6FNkknoBvwZ6A+1ryiPigBTrlVcdOmR+u3dv3XqYmRWTpjxFdidwC1ANHEHms8m/a3CNbUxJSebX78GYmRVOUwJm54h4nMwTZ69HxCXAkelWK792StppH37YqtUwMysqTXnR8lNJbYBXk65f3qKeLvK3VW7BmJkVXlNaMOcDuwDjgYHAN9jU2eR2oaQk8ySZWzBmZoXTYAsm6WzyaxFxAbAW+FZBapWCPfd0C8bMrJDqbcFI2ikiNgADa7rU3561bw+zZsHs2a1dEzOz4tBQC+YFYADwIvCQpD8AH9fMjIg/ply3vPn4Y3jnHVi+HIYNc3cxZmaF0JSb/J2AVWSeHAsyn0IOYLsJmI8+yrzJD+4uxsysUBoKmL0l/QhYyKZgqbFd9ZC5226ZR5Wrq6FtW3cXY2ZWCA09RVYC7JoMu2WN1wzbjQ4d4PzzM+OTJ7v1YmZWCA21YJZHxKUFq0nKhg+Ha66B//kf6NLFIWNmlraGWjAtfnJM0rGSlkhaKmlCjvlDJa2WND8ZLk7K95f0pKSXJS2S9IOsdS6R9FbWOsc3pS6rV2d+b7stc6PfT5OZmaWroRbMsJZsOHmH5mZgOFAFzJE0LSIW11l0ZkSMqFNWDfw4IuZJ2g2YK+nRrHWvi4it+ozAq69mfiPgs898o9/MLG31tmAi4v0WbnsQsDQiXouIdcBk4OSmrBgRyyNiXjL+EfAysF9LKtO586bxjRthr71asjUzM2tMU7qKaa79gDezpqvIHRIVkl6S9LCkPnVnSuoB9AeezyoeJ2mBpDskdcy1c0ljJFVKqly5ciWrVmW6i8nMgxdfbN5BmZlZ06QZMLnu4dR9vHke0D0i+gE3AlM324C0K/AgcH5ErEmKbwE+D5QBy4H/yLXziJgYEeURUd6lSxeGDs180TIzD+680/dhzMzSlGbAVAH7Z013A97OXiAi1kTE2mR8OlAqqTOApFIy4TIpu9eAiFgRERsiYiNwG5lLcY2qqIBzztk0vW4d3HNPcw7LzMyaIs2AmQP0ktRTUltgJDAtewFJ+9b0cyZpUFKfVUnZ7cDLEXFtnXW6Zk2eSuZF0CY5++zNWzG33+5WjJlZWlILmIioBsYBj5C5SX9/RCySNFbS2GSx04GFkl4CbgBGRkQAhwHfBI7M8TjyVZL+KmkBmS9s/rCpdaqogBNO2DS9fj1cdVXLjtPMzHJrSl9kzZZc9ppep+zWrPGbgJtyrDeLet7DiYhvtqRO++67+fRDD8HEiTBmTEu2amZmdaV5iWybdPbZm75wCZlLZd/7ni+VmZnlW9EFTEUF/Nd/bXpkGWDDBl8qMzPLt6ILGMhcDju5ziufNZfKzMwsP4oyYAAuvNCXyszM0lS0AeNLZWZm6SragIHcl8qmToWLLmqV6piZ7VCKOmBgy0tlkGnFOGTMzFqm6AMm16UygKuv9k1/M7OWKPqAgcylsgsu2LzMN/3NzFrGAZO48srM5bJsGzbAd77jkDEzaw4HTJYrr4RTTtm8bPFiGDLEIWNmtrUcMHXkuum/fr1bMmZmW8sBU0d9N/3dkjEz2zoOmBzGjIFbb90yZNySMTNrOgdMPeoLmcWL4fDD/QizmVljHDANqC9kNm6EsWMdMmZmDXHANKK+kIlwyJiZNcQB0wQ1IdOmztmKgO9+193KmJnlkmrASDpW0hJJSyVNyDF/qKTVkuYnw8VJ+f6SnpT0sqRFkn6QtU4nSY9KejX57ZjmMdQYMwZmzYLevbecd9VVfsLMzKyu1AJGUglwM3Ac0Bs4S1KOP8/MjIiyZLg0KasGfhwRBwGDge9nrTsBeDwiegGPJ9MFUVEBv/0tlJZuOe/pp+Gww+DUUx00ZmaQbgtmELA0Il6LiHXAZODkRtYBICKWR8S8ZPwj4GVgv2T2ycDdyfjdwCn5rHRjKirgqafgq1/dcl5Eprt/P2VmZpZuwOwHvJk1XcWmkMhWIeklSQ9L6lN3pqQeQH/g+aRon4hYDpkgAvbOtXNJYyRVSqpcuXJlCw4jR4WTkKnbd1mNjRsz92Z82czMilmaAaMcZVFneh7QPSL6ATcCUzfbgLQr8CBwfkSs2ZqdR8TEiCiPiPIuXbpszapNduWV8JvfbHnzv0bNZTM/BGBmxSjNgKkC9s+a7ga8nb1ARKyJiLXJ+HSgVFJnAEmlZMJlUkT8MWu1FZK6Jst0Bd5N7xAaV3Pz/5RTtnyUGTKXza66Crp29f0ZMysuaQbMHKCXpJ6S2gIjgWnZC0jaV8r8WZY0KKnPqqTsduDliLi2znanAaOS8VHAQykeQ5NUVMCUKfDMM7nvzQC8807m/syXv+xLZ2ZWHFILmIioBsYBj5C5SX9/RCySNFbS2GSx04GFkl4CbgBGRkQAhwHfBI7MeoT5+GSdK4Dhkl4FhifT24SaezO/+Q10717/ck8/nQma/v3h3HMdNma2Y1Lm7/mOrby8PCorKwu+34suylwea4reveEHP8hccjMz2xZImhsR5c1d32/yp+jKK+HZZzP3Z/bdt+FlFy/OPHnWtWvmEppbNma2vXPApKzm/szy5Y1fOoPMvZqnn850TfPlL0PPnn44wMy2Tw6YAhozBpYtywTNQQflfuqsrmXLNj0c0LNn5r5N794OHTPb9vkeTCuaPRvuuQeeew7mz2/eNnr1gp12gnbtoG1b+Pa3fR/HzPKjpfdgHDDbiNmzMw8EvPgivPFG5v2Z5tp338zw2WeZ4PnsMzjwwEzPAxUV+auzme3YHDBNsD0ETLaals3ixfD665khXz7/+UxLpyZ46v46iMyshgOmCba3gKmrpnWzZEkmCN55JzOkqXt3aN8edt45dxA19tulS+Ze0dlnO6zMtlcOmCbY3gMml4kT4fbbYd26zB/06mp49dXWrlVuXbvC7rtn6tiu3Za/zQmwpgQcwMqV6e1jR/v1OfO5qvs7f/4hn0b8defm/rdfHAGz225ROXBga1cjdbNX9+aqN89kySefox3r+Iy2tGMd76zvxDvrO2/FlprweFve7Pj//pltvw4lorLZfxB2ymdVrHVV7LGYKXv8Iue8iW+fwO3vnMC6DSW1wVP3d1MQ5euPflP+vSxkmJlZIRVHwBx4IMyY0dq1aFVjkqExdS+9NadZ/cEHLX8Szsy2f8URMNZkY8bk5z2a7CfhWuM6dbFcI/c587lK9x7MZ5+25O+AA8ZSUVHhp8fMtnfSwkUtWd9dxZiZWSocMGZmlgoHjJmZpcIBY2ZmqXDAmJlZKlINGEnHSloiaamkCTnmD5W0WtL8ZLg4a94dkt6VtLDOOpdIeitrnePTPAYzM2ue1B5TllQC3AwMB6qAOZKmRcTiOovOjIgROTZxF3ATcE+OeddFxDX5rK+ZmeVXmi2YQcDSiHgtItYBk4GTm7pyRDwNvJ9W5czMLF1pBsx+wJtZ01VJWV0Vkl6S9LCkPk3c9jhJC5LLaB1bXFMzM8u7NAMmVy+GdXunmgd0j4h+wI3A1CZs9xbg80AZsBz4j5w7l8ZIqpRUuXLlyqbW2czM8iTNgKkC9s+a7ga8nb1ARKyJiLXJ+HSgVFKD/cpHxIqI2BARG4HbyFyKy7XcxIgoj4jyLjUdB5mZWcGkGTBzgF6SekpqC4wEpmUvIGlfSUrGByX1WdXQRiV1zZo8FVhY37JmZtZ6UnuKLCKqJY0DHgFKgDsiYpGkscn8W4HTgXMlVQOfACMj+QKapHuBoUBnSVXALyLiduAqSWVkLrctA76b1jGYmVnzFccXLXfATyabmaVN0tyIKG/u+n6T38zMUuGAMTOzVDhgzMwsFQ4YMzNLhQPGzMxS4YAxM7NUOGDMzCwVDhgzM0uFA8bMzFLhgDEzs1Q4YMzMLBUOGDMzS4UDxszMUuGAMTOzVDhgzMwsFQ4YMzNLhQPGzMxS4YAxM7NUOGDMzCwVqQaMpGMlLZG0VNKEHPOHSlotaX4yXJw17w5J70paWGedTpIelfRq8tsxzWMwM7PmSS1gJJUANwPHAb2BsyT1zrHozIgoS4ZLs8rvAo7NsfwE4PGI6AU8nkybmdk2Js0WzCBgaUS8FhHrgMnAyU1dOSKeBt7PMetk4O5k/G7glBbW08zMUrBTitveD3gza7oK+FKO5SokvQS8DfwkIhY1st19ImI5QEQsl7R3roUkjQHGJJOf1b3UVsQ6A++1diW2ET4Xm/hcbOJzscmBLVk5zYBRjrKoMz0P6B4RayUdD0wFeuVj5xExEZgIIKkyIsrzsd3tnc/FJj4Xm/hcbOJzsYmkypasn+Ylsipg/6zpbmRaKbUiYk1ErE3GpwOlkjo3st0VkroCJL/v5q/KZmaWL2kGzBygl6SektoCI4Fp2QtI2leSkvFBSX1WNbLdacCoZHwU8FBea21mZnmRWsBERDUwDngEeBm4PyIWSRoraWyy2OnAwuQezA3AyIgIAEn3ArOBAyVVSfp2ss4VwHBJrwLDk+nGTMzbgW3/fC428bnYxOdiE5+LTVp0LpT8PTczM8srv8lvZmapcMCYmVkqduiAaayrmh1Nru51GupaR9JPk3OzRNIxrVPrdEjaX9KTkl6WtEjSD5LyojsfktpLekHSS8m5+GVSXnTnooakEkkvSvpzMl2U50LSMkl/TbrqqkzK8ncuImKHHIAS4O/AAUBb4CWgd2vXK+Vj/iowAFiYVXYVMCEZnwBcmYz3Ts5JO6Bncq5KWvsY8nguugIDkvHdgL8lx1x054PMO2m7JuOlwPPA4GI8F1nn5EfAfwN/TqaL8lwAy4DOdcrydi525BZMi7qq2R5F7u516uta52RgckR8FhH/Bywlc852CBGxPCLmJeMfkXmScT+K8HxExtpksjQZgiI8FwCSugEnAL/NKi7Kc1GPvJ2LHTlgcnVVs18r1aU1bda1DlDTtU7RnB9JPYD+ZP7PvSjPR3JJaD6ZF5MfjYiiPRfA9cCFwMassmI9FwH8RdLcpHstyOO5SLOrmNbWlK5qillRnB9JuwIPAudHxJrkvd6ci+Yo22HOR0RsAMok7QlMkXRwA4vvsOdC0gjg3YiYK2loU1bJUbZDnIvEYRHxdtKn46OSXmlg2a0+FztyC6bRrmqKRH1d6+zw50dSKZlwmRQRf0yKi/Z8AETEh8AMMp/CKMZzcRhwkqRlZC6bHynp9xTnuSAi3k5+3wWmkLnklbdzsSMHTKNd1RSJ+rrWmQaMlNROUk8ynYy+0Ar1S0XSBdHtwMsRcW3WrKI7H5K6JC0XJO0MHAW8QhGei4j4aUR0i4geZP4mPBER36AIz4WkDpJ2qxkHjgYWks9z0dpPMaT8hMTxZJ4e+jvw89auTwGO915gObCezP9tfBvYi8yH2V5NfjtlLf/z5NwsAY5r7frn+VwcTqb5vgCYnwzHF+P5APoCLybnYiFwcVJedOeiznkZyqanyIruXJB5wvalZFhU8zcyn+fCXcWYmVkqduRLZGZm1oocMGZmlgoHjJmZpcIBY2ZmqXDAmJlZKhwwZltB0trkt4ekr+d52z+rM/1sPrdvVmgOGLPm6QFsVcBIKmlkkc0CJiK+vJV1MtumOGDMmucK4CvJdzR+mHQmebWkOZIWSPougKShyXdp/hv4a1I2NelccFFNB4OSrgB2TrY3KSmraS0p2fbC5NsdZ2Zte4akByS9ImmSGuhszazQduTOLs3SNAH4SUSMAEiCYnVEHCqpHfCMpL8kyw4CDo5MF+cA50TE+0m3LXMkPRgREySNi4iyHPs6DSgD+gGdk3WeTub1B/qQ6RPqGTJ9bc3K98GaNYdbMGb5cTRwdtIl/vNkutvolcx7IStcAMZLegl4jkzngb1o2OHAvRGxISJWAE8Bh2ZtuyoiNpLpDqdHHo7FLC/cgjHLDwHnRcQjmxVmuoT/uM70UUBFRPxD0gygfRO2XZ/PssY34P+mbRviFoxZ83xE5lPMNR4Bzk0+EYCkLyQ91Na1B/BBEi5fJPPp4hrra9av42ngzOQ+Txcyn8beIXr0tR2b/2/HrHkWANXJpa67gP8kc3lqXnKjfSWbPjWb7X+BsZIWkOmR9rmseROBBZLmRcS/ZpVPASrI9HobwIUR8U4SUGbbLPembGZmqfAlMjMzS4UDxszMUuGAMTOzVDhgzMwsFQ4YMzNLhQPGzMxS4YAxM7NU/H99O7qTn3t5dQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adicionando atributos\n",
    "\n",
    "1. Adicione atributos polinomiais de grau `d=2` usando o transformador `PolynomialFeatures`. Em seguida, repita o treinamento via solução analítica e estimação do grau de condicionamento da hessiana.\n",
    "- Repita o treinamento usando método do gradiente (incluindo gráfico da função custo) e verifique a dificuldade de convergência. Por que isso ocorre? Foi necessário alterar a taxa de aprendizado? E o número máximo de iterações?\n",
    "- Assim como anteriormente, compare o MSE e o $\\bw$ obtidos com os da solução analítica.\n",
    "- Repita os itens anteriores para `d=3`.\n",
    "\n",
    "#### Dica\n",
    "\n",
    "- Não há necessidade de incluir o termo constante nos atributos adicionados, uma vez que o modelo de regressão linear já implementa a adição de coluna de 1's. Assim, utilize `PolynomialFeatures(d, include_bias=False)`.\n",
    "- Normalmente, é conveniente utilizar a função `make_pipeline` para combinar pré-processamento (transformação de atributos) e modelo de aprendizado (estimador) em um único modelo. Além de deixar o código mais compacto, essa metodologia ajuda a evitar erros de vazamento de informação entre teste e treinamento, pois garante que o transformador será treinado somente com os dados de treinamento. No entanto, como o nosso foco aqui é o treinamento, é mais conveniente primeiramente aplicar a transformação de atributos explicitamente no conjunto de dados, obtendo um conjunto transformado (aqui com sufixo `_new`), o qual é então entregue ao modelo de aprendizado. Embora não seja o caso aqui, essa abordagem também é mais eficiente quando o pré-processamento é particularmente complexo e serão realizados múltiplos treinamentos, assim o pré-processamento só precisa ser realizado uma vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [-0.35250239  0.26848578 -0.06441263]\n",
      "Train MSE: 0.511996\n",
      "  Val MSE: 0.611803\n",
      "Condition number: 955.280910\n"
     ]
    }
   ],
   "source": [
    "# Feature transformation\n",
    "d = 2\n",
    "prep = PolynomialFeatures(d,include_bias=False)\n",
    "X_new = prep.fit_transform(X)\n",
    "X_val_new = prep.transform(X_val)\n",
    "\n",
    "# Normal equation\n",
    "model = Model()\n",
    "model.fit(X_new, y)\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Escalonamento de atributos\n",
    "\n",
    "Implemente a normalização (escalonamento) de atributos conforme vista em sala, a qual consiste de:\n",
    " - Subtração da média do atributo, para que passe a ter média nula\n",
    " - Divisão pelo desvio padrão do atributo, para que passe a ter variância unitária\n",
    " \n",
    "Esse tipo de normalização também é chamado em alguns contextos de padronização (*standardization*), no sentido de resultar na mesma média (0) e variância (1) de uma variável aleatória gaussiana padrão (*standard*), em contraste com outros tipos de normalização (por exemplo, reescalonamento para a escala [0,1]).\n",
    "\n",
    "1. Para isso, complete a classe abaixo. Caso deseje confirmar se sua implementação está correta, compare com o transformador `StandardScaler` do módulo `sklearn.preprocessing`.\n",
    "- Após implementar corretamente, verifique que seu escalonador funciona corretamente em um pipeline do `sklearn`; isto é, combine todas as etapas de pré-processamento (transformação de atributos e escalonamento) usando `make_pipeline`. Em seguida, você pode ignorar sua implementação e passar a usar o `StandardScaler`.\n",
    "- Refaça os mesmos passos da seção anterior (2.1-2.4) e compare os resultados e o comportamento do algoritmo. Explique.\n",
    "- Neste problema, em qual posição o escalonador funciona melhor, antes ou depois da adição de atributos? Cite as evidências que você observou.\n",
    "- O uso do escalonador tem impacto do desempenho da solução analítica? Por quê?\n",
    "- (OPCIONAL) Experimente outros escalonadores do `sklearn`, como `MinMaxScaler` e `MaxAbsScaler`, e compare o desempenho obtido.\n",
    "\n",
    "#### Dicas\n",
    "\n",
    "- Funções úteis:\n",
    "\n",
    "```python\n",
    "np.mean(axis=0), np.std(axis=0)\n",
    "```\n",
    "\n",
    "- Revise as propriedades de broadcasting do NumPy, em particular em operações envolvendo matrizes e arrays 1D.\n",
    "- Para depurar possíveis erros, lembre-se de verificar o `shape` dos arrays envolvidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.34164079, -1.14458618, -0.98184354],\n",
       "       [-0.4472136 , -0.61631563, -0.69547251],\n",
       "       [ 0.4472136 ,  0.26413527,  0.08182029],\n",
       "       [ 1.34164079,  1.49676654,  1.59549575]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class MyStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Compute and store scaler parameters\n",
    "        ???\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # Scale features\n",
    "        ???\n",
    "        return X_new\n",
    "\n",
    "A = np.array([[1, 1, 1], [2, 4, 8], [3, 9, 27], [4, 16, 64]])\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ainda mais atributos\n",
    "\n",
    "1. Adicione atributos polinomiais de grau ainda maior (ex: d=4, d=5). O que você observa?\n",
    "- Você recomendaria o método do gradiente para um problema desse tipo? Ou seria melhor usar um método de segunda ordem? Explique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conjunto de dados #2\n",
    "\n",
    "O segundo conjunto de dados que usaremos consiste de dados sobre a venda de casas em King County, USA, entre maio de 2014 e maio de 2015. O objetivo é prever o preço de venda a partir de informações sobre a casa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21613, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Original source: http://www.kaggle.com/harlfoxem/housesalesprediction/data\n",
    "df = pd.read_csv('https://github.com/danilo-silva-ufsc/ml/raw/master/data/kc_house_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como variável de saída, $y$, utilize o **logaritmo** neperiano do preço de venda , `price`, i.e., `np.log(price)`. Desta forma o erro na predição de $y$ será função do erro _relativo_ na predição do preço, evitando dar peso excessivo aos preços mais altos. Por exemplo, quando a função perda é o erro quadrático, a perda equivale aproximadamente ao quadrado do erro relativo:\n",
    "\n",
    "$L(y,\\hat{y}) = (\\hat{y} - y)^2 = (\\log(\\hat{p}) - \\log(p))^2 = (\\log(\\hat{p}/p))^2 = (\\log(1 + (\\hat{p}-p)/p))^2 \\approx ((\\hat{p}-p)/p)^2$\n",
    "\n",
    "\n",
    "Como atributos, utilize apenas as 4 colunas após o preço de venda, i.e.:\n",
    "- `bedrooms`: número de quartos\n",
    "- `bathrooms`: número de quartos, em múltiplos de 1/4 (https://www.realtor.com/advice/buy/what-is-a-half-bath/)\n",
    "- `sqft_living`: área da casa, em ft${^2}$\n",
    "- `sqft_lot`: área do lote, em ft${^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação\n",
    "\n",
    "1. Prepare e divida o conjunto de dados aleatoriamente em conjuntos de treinamento, validação e teste, nas proporções 60%, 20% e 20%, respectivamente. Para isso, utilize a função `sklearn.model_selection.train_test_split()`.\n",
    "- Como função perda para o treinamento, utilize o erro quadrático, e, como métrica de avaliação do modelo, utilize a raiz quadrada do erro quadrático médio. Ambos são equivalentes, mas o segundo resulta em valores numa escala mais agradável para análise e mais fácil de interpretar. Adicionalmente, utilize como métrica de avaliação o [erro percentual absoluto médio](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) (MAPE) do preço de venda (i.e., da variável original `price`, **não** da variável `y = np.log(price)`). Esta métrica é ainda mais fácil de interpretar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21560, 4) (21560,)\n"
     ]
    }
   ],
   "source": [
    "# Removing outliers\n",
    "df = df[df['bedrooms'] < 10]\n",
    "df = df[df['bathrooms'] < 6]\n",
    "df = df[df['sqft_living'] < 7000]\n",
    "df = df[df['sqft_lot'] < 600e3]\n",
    "\n",
    "X = df[['bedrooms','bathrooms','sqft_living','sqft_lot']].to_numpy()\n",
    "y = ???\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12936, 4) (12936,)\n",
      "(4312, 4) (4312,)\n",
      "(4312, 4) (4312,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(???, test_size=y_test.shape[0], random_state=0)\n",
    "del(X,y) # just to make sure we will not use them by mistake. Or set X,y = X_train,y_train\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(model, X, y):\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(model, X, y):\n",
    "    p = ???\n",
    "    p_pred = ???\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploração dos dados\n",
    "\n",
    "Antes de escolher e começar a treinar um modelo, é útil fazer uma breve exploração dos dados. (Foi dessa exploração inicial que surgiu a ideia, por exemplo, de remover outliers, com aqueles valores específicos.) \n",
    "\n",
    "3. Para cada atributo, trace o gráfico da variável de saída em função do atributo, **sobre o conjunto de treinamento** (não trace gráficos sobre o conjunto de teste para evitar vazamento de informação). Observe as escalas das variáveis envolvidas e analise se há alguma dependência aparente entre as variáveis. Intuitivamente, qual atributo parece ser mais preditivo do preço do venda?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regressão linear\n",
    "\n",
    "4. Inicialmente você deve treinar um modelo de regressão linear sem regularização e calcular o desempenho da predição (RMSE e MAPE) sobre o conjunto de treinamento e sobre o conjunto de validação. Fique à vontade para usar as funções do `sklearn`, não há necessidade de usar o método do gradiente.\n",
    "- Você diria que o modelo treinado sofre de underfitting, overfitting ou nenhum dos dois? Explique.\n",
    "- Analisando o vetor de pesos do modelo treinado (`model.coef_`), qual atributo você diria que é o mais importante para a predição? Por quê? Esta observação confirma a sua hipótese do item anterior? Explique.\n",
    "\n",
    "#### Dica\n",
    "- Para acessar o regressor dentro de um *pipeline* do sklearn, inicialize-o fora do *pipeline* ou acesse-o via `model.steps[-1][1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aprimorando o modelo\n",
    "\n",
    "6. Usando o que vimos até agora na disciplina, tente ao máximo melhorar o desempenho do modelo neste conjunto de dados. Reporte o desempenho obtido (RMSE e MAPE).\n",
    "\n",
    "#### Dica:\n",
    "- Reveja os conceitos aprendidos na Aula 2 e no Exercício 2.\n",
    "- Se desejar aplicar alguma transformação de atributos \"customizada\", você tem duas opções: criar um transformador customizado do `sklearn` e integrá-lo em uma *pipeline* (ver último item opcional do Exercício 2), ou, *somente se for uma transformação que não envolve estimação de parâmetros*, você pode aplicá-la diretamente a todo o conjunto de dados (matrix $\\bX$ antes do *split*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (OPCIONAL)\n",
    "\n",
    "- Tente utilizar mais colunas da tabela original para melhorar o desempenho.\n",
    "- Utilize um outro conjunto de dados com múltiplos atributos. Sugestão: https://archive.ics.uci.edu/ml/datasets/Wine+Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
